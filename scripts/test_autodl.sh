#!/bin/bash
# AutoDL æµ‹è¯•è„šæœ¬
# ä½¿ç”¨æ–¹æ³•: bash scripts/test_autodl.sh [dataset] [checkpoint_path]
# ç¤ºä¾‹: bash scripts/test_autodl.sh mvtec checkpoints/mvtec_20231201_120000

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

print_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# é»˜è®¤å‚æ•°
DATASET=${1:-"mvtec"}
CHECKPOINT_PATH=${2:-""}
BATCH_SIZE=${3:-32}
NUM_WORKERS=${4:-4}

echo "ğŸ§ª å¼€å§‹åœ¨ AutoDL ä¸Šæµ‹è¯• Experience Replay æ¨¡å‹"
echo "================================================"
print_info "æ•°æ®é›†: $DATASET"
print_info "æ£€æŸ¥ç‚¹è·¯å¾„: $CHECKPOINT_PATH"
print_info "æ‰¹æ¬¡å¤§å°: $BATCH_SIZE"
print_info "å·¥ä½œè¿›ç¨‹: $NUM_WORKERS"
echo "================================================"

# æ£€æŸ¥ç¯å¢ƒ
check_environment() {
    print_info "æ£€æŸ¥æµ‹è¯•ç¯å¢ƒ..."
    
    # æ£€æŸ¥ CUDA
    if ! command -v nvidia-smi &> /dev/null; then
        print_error "æœªæ‰¾åˆ° CUDA ç¯å¢ƒ"
        exit 1
    fi
    
    # æ£€æŸ¥ GPU çŠ¶æ€
    GPU_MEMORY=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    GPU_USED=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits | head -1)
    print_info "GPU å†…å­˜: ${GPU_USED}MB / ${GPU_MEMORY}MB"
    
    print_success "ç¯å¢ƒæ£€æŸ¥å®Œæˆ"
}

# è®¾ç½®ç¯å¢ƒå˜é‡
setup_test_env() {
    print_info "è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡..."
    
    export CUDA_VISIBLE_DEVICES=0
    export PYTHONPATH=$PYTHONPATH:$(pwd)
    export TORCH_CUDNN_V8_API_ENABLED=1
    
    # æµ‹è¯•ä¼˜åŒ–è®¾ç½®
    export OMP_NUM_THREADS=4
    export MKL_NUM_THREADS=4
    
    print_success "ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ"
}

# åˆ›å»ºæµ‹è¯•ç›®å½•
setup_test_dirs() {
    print_info "åˆ›å»ºæµ‹è¯•ç›®å½•..."
    
    TIMESTAMP=$(date +%Y%m%d_%H%M%S)
    TEST_NAME="test_${DATASET}_${TIMESTAMP}"
    
    mkdir -p results/$TEST_NAME
    mkdir -p logs/$TEST_NAME
    
    export TEST_DIR="results/$TEST_NAME"
    export LOG_DIR="logs/$TEST_NAME"
    
    print_success "æµ‹è¯•ç›®å½•åˆ›å»ºå®Œæˆ: $TEST_NAME"
}

# æ£€æŸ¥æ•°æ®é›†
check_dataset() {
    print_info "æ£€æŸ¥æµ‹è¯•æ•°æ®é›†: $DATASET"
    
    case $DATASET in
        "mvtec")
            DATA_DIR="data/MVTec-AD"
            if [ ! -d "$DATA_DIR" ]; then
                print_error "MVTec-AD æ•°æ®é›†æœªæ‰¾åˆ°"
                exit 1
            fi
            ;;
        "visa")
            DATA_DIR="data/VisA"
            if [ ! -d "$DATA_DIR" ]; then
                print_error "VisA æ•°æ®é›†æœªæ‰¾åˆ°"
                exit 1
            fi
            ;;
        *)
            print_error "ä¸æ”¯æŒçš„æ•°æ®é›†: $DATASET"
            exit 1
            ;;
    esac
    
    # æ£€æŸ¥æµ‹è¯•æ•°æ®
    TEST_DATA_COUNT=$(find $DATA_DIR -name "*.png" -o -name "*.jpg" | wc -l)
    print_info "æ‰¾åˆ° $TEST_DATA_COUNT ä¸ªæµ‹è¯•å›¾åƒ"
    
    print_success "æ•°æ®é›†æ£€æŸ¥å®Œæˆ: $DATA_DIR"
}

# æ£€æŸ¥æ£€æŸ¥ç‚¹
check_checkpoint() {
    if [ -z "$CHECKPOINT_PATH" ]; then
        print_info "æœªæŒ‡å®šæ£€æŸ¥ç‚¹ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹..."
        
        # æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
        LATEST_CHECKPOINT=$(find checkpoints -name "*${DATASET}*" -type d | sort | tail -1)
        
        if [ -z "$LATEST_CHECKPOINT" ]; then
            print_error "æœªæ‰¾åˆ° $DATASET çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"
            print_info "è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æŒ‡å®šæ£€æŸ¥ç‚¹è·¯å¾„"
            exit 1
        fi
        
        CHECKPOINT_PATH="$LATEST_CHECKPOINT"
        print_info "ä½¿ç”¨æœ€æ–°æ£€æŸ¥ç‚¹: $CHECKPOINT_PATH"
    fi
    
    print_info "æ£€æŸ¥æ£€æŸ¥ç‚¹: $CHECKPOINT_PATH"
    
    if [ ! -d "$CHECKPOINT_PATH" ]; then
        print_error "æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: $CHECKPOINT_PATH"
        exit 1
    fi
    
    # æŸ¥æ‰¾æ£€æŸ¥ç‚¹æ–‡ä»¶
    CHECKPOINT_FILES=$(find $CHECKPOINT_PATH -name "*.ckpt" -o -name "*.pth" -o -name "*.pt" | wc -l)
    if [ "$CHECKPOINT_FILES" -eq 0 ]; then
        print_error "æ£€æŸ¥ç‚¹ç›®å½•ä¸­æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶"
        exit 1
    fi
    
    print_success "æ£€æŸ¥ç‚¹æ£€æŸ¥å®Œæˆ"
}

# åˆ›å»ºæµ‹è¯•é…ç½®
create_test_config() {
    print_info "åˆ›å»ºæµ‹è¯•é…ç½®..."
    
    cat > $LOG_DIR/test_config.yaml << EOF
# AutoDL æµ‹è¯•é…ç½®
test_name: $TEST_NAME
dataset: $DATASET
checkpoint_path: $CHECKPOINT_PATH
timestamp: $(date)

# æµ‹è¯•å‚æ•°
batch_size: $BATCH_SIZE
num_workers: $NUM_WORKERS

# è·¯å¾„é…ç½®
paths:
  data_dir: $DATA_DIR
  checkpoint_path: $CHECKPOINT_PATH
  result_dir: $TEST_DIR
  log_dir: $LOG_DIR

# è¯„ä¼°æŒ‡æ ‡
metrics:
  - auroc
  - aupr
  - f1_score
  - accuracy
  - precision
  - recall
EOF
    
    print_success "æµ‹è¯•é…ç½®åˆ›å»ºå®Œæˆ"
}

# è¿è¡Œæµ‹è¯•
run_test() {
    print_info "å¼€å§‹æµ‹è¯•æ¨¡å‹..."
    
    # åˆ›å»ºæµ‹è¯•è„šæœ¬
    cat > $LOG_DIR/run_test.py << 'EOF'
import os
import sys
import torch
import numpy as np
from pathlib import Path
import yaml
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.getcwd())

try:
    from cdm.gpm_experience_replay import CDAD_ExperienceReplay
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥ CDAD_ExperienceReplayï¼Œä½¿ç”¨åŸºç¡€æµ‹è¯•")
    CDAD_ExperienceReplay = None

def load_test_config():
    """åŠ è½½æµ‹è¯•é…ç½®"""
    config_path = sys.argv[1] if len(sys.argv) > 1 else 'test_config.yaml'
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def test_model_loading(checkpoint_path):
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("ğŸ” æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    # æŸ¥æ‰¾æ£€æŸ¥ç‚¹æ–‡ä»¶
    checkpoint_files = list(Path(checkpoint_path).glob('*.ckpt')) + \
                      list(Path(checkpoint_path).glob('*.pth')) + \
                      list(Path(checkpoint_path).glob('*.pt'))
    
    if not checkpoint_files:
        print("âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶")
        return False
    
    checkpoint_file = checkpoint_files[0]
    print(f"ğŸ“ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_file}")
    
    try:
        # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(checkpoint_file, map_location='cpu')
        print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥æ£€æŸ¥ç‚¹å†…å®¹
        if isinstance(checkpoint, dict):
            keys = list(checkpoint.keys())
            print(f"ğŸ“‹ æ£€æŸ¥ç‚¹åŒ…å«é”®: {keys[:5]}{'...' if len(keys) > 5 else ''}")
        
        return True
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
        return False

def test_experience_replay(config):
    """æµ‹è¯• Experience Replay åŠŸèƒ½"""
    print("ğŸ§  æµ‹è¯• Experience Replay åŠŸèƒ½...")
    
    if CDAD_ExperienceReplay is None:
        print("âš ï¸  è·³è¿‡ Experience Replay æµ‹è¯•ï¼ˆæ¨¡å—æœªæ‰¾åˆ°ï¼‰")
        return {}
    
    try:
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = CDAD_ExperienceReplay()
        
        # é…ç½® Experience Replay
        model.configure_experience_replay(
            buffer_size=1000,
            projection_dim=64,
            update_frequency=10,
            similarity_threshold=0.8
        )
        
        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        test_input = torch.randn(4, 3, 224, 224)
        
        with torch.no_grad():
            output = model(test_input)
        
        print(f"âœ… Experience Replay æµ‹è¯•é€šè¿‡")
        print(f"ğŸ“Š è¾“å‡ºå½¢çŠ¶: {output.shape if hasattr(output, 'shape') else type(output)}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = model.get_statistics()
        print(f"ğŸ“ˆ Experience Replay ç»Ÿè®¡: {stats}")
        
        return {
            'status': 'success',
            'output_shape': str(output.shape) if hasattr(output, 'shape') else str(type(output)),
            'statistics': stats
        }
        
    except Exception as e:
        print(f"âŒ Experience Replay æµ‹è¯•å¤±è´¥: {e}")
        return {
            'status': 'failed',
            'error': str(e)
        }

def test_data_loading(data_dir, batch_size):
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("ğŸ“‚ æµ‹è¯•æ•°æ®åŠ è½½...")
    
    try:
        from torch.utils.data import DataLoader
        from torchvision import transforms, datasets
        
        # åˆ›å»ºæ•°æ®å˜æ¢
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # å°è¯•åŠ è½½æ•°æ®
        if os.path.exists(os.path.join(data_dir, 'test')):
            test_dataset = datasets.ImageFolder(
                root=os.path.join(data_dir, 'test'),
                transform=transform
            )
        else:
            # å¦‚æœæ²¡æœ‰æ ‡å‡†ç»“æ„ï¼Œå°è¯•ç›´æ¥åŠ è½½å›¾åƒ
            image_files = list(Path(data_dir).rglob('*.png')) + \
                         list(Path(data_dir).rglob('*.jpg'))
            print(f"ğŸ“ æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
            
            if len(image_files) == 0:
                print("âš ï¸  æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
                return {'status': 'no_data'}
            
            return {
                'status': 'success',
                'image_count': len(image_files),
                'sample_files': [str(f) for f in image_files[:3]]
            }
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=2
        )
        
        # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
        batch = next(iter(test_loader))
        images, labels = batch
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š æ‰¹æ¬¡å½¢çŠ¶: {images.shape}")
        print(f"ğŸ·ï¸  æ ‡ç­¾æ•°é‡: {len(torch.unique(labels))}")
        
        return {
            'status': 'success',
            'dataset_size': len(test_dataset),
            'batch_shape': str(images.shape),
            'num_classes': len(torch.unique(labels)).item()
        }
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return {
            'status': 'failed',
            'error': str(e)
        }

def generate_test_report(config, results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    report = {
        'test_info': {
            'test_name': config['test_name'],
            'dataset': config['dataset'],
            'timestamp': datetime.now().isoformat(),
            'checkpoint_path': config['checkpoint_path']
        },
        'results': results,
        'summary': {
            'total_tests': len(results),
            'passed_tests': sum(1 for r in results.values() if r.get('status') == 'success'),
            'failed_tests': sum(1 for r in results.values() if r.get('status') == 'failed')
        }
    }
    
    # ä¿å­˜ JSON æŠ¥å‘Š
    result_dir = config['paths']['result_dir']
    with open(os.path.join(result_dir, 'test_report.json'), 'w') as f:
        json.dump(report, f, indent=2)
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    md_report = f"""# æµ‹è¯•æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- æµ‹è¯•åç§°: {config['test_name']}
- æ•°æ®é›†: {config['dataset']}
- æ£€æŸ¥ç‚¹: {config['checkpoint_path']}
- æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æµ‹è¯•ç»“æœ

### æ€»è§ˆ
- æ€»æµ‹è¯•æ•°: {report['summary']['total_tests']}
- é€šè¿‡æµ‹è¯•: {report['summary']['passed_tests']}
- å¤±è´¥æµ‹è¯•: {report['summary']['failed_tests']}

### è¯¦ç»†ç»“æœ
"""
    
    for test_name, result in results.items():
        status_icon = "âœ…" if result.get('status') == 'success' else "âŒ"
        md_report += f"\n#### {status_icon} {test_name}\n"
        
        if result.get('status') == 'success':
            for key, value in result.items():
                if key != 'status':
                    md_report += f"- {key}: {value}\n"
        else:
            md_report += f"- é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}\n"
    
    with open(os.path.join(result_dir, 'test_report.md'), 'w') as f:
        f.write(md_report)
    
    return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª å¼€å§‹æ¨¡å‹æµ‹è¯•")
    print("=" * 50)
    
    # åŠ è½½é…ç½®
    config = load_test_config()
    
    # è¿è¡Œæµ‹è¯•
    results = {}
    
    # æµ‹è¯•1: æ¨¡å‹åŠ è½½
    results['model_loading'] = {
        'status': 'success' if test_model_loading(config['checkpoint_path']) else 'failed'
    }
    
    # æµ‹è¯•2: æ•°æ®åŠ è½½
    results['data_loading'] = test_data_loading(
        config['paths']['data_dir'],
        config['batch_size']
    )
    
    # æµ‹è¯•3: Experience Replay
    results['experience_replay'] = test_experience_replay(config)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_test_report(config, results)
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“Š é€šè¿‡: {report['summary']['passed_tests']}/{report['summary']['total_tests']}")
    print(f"ğŸ“ æŠ¥å‘Šä¿å­˜è‡³: {config['paths']['result_dir']}")

if __name__ == '__main__':
    main()
EOF
    
    # è¿è¡Œæµ‹è¯•
    print_info "æ‰§è¡Œæµ‹è¯•è„šæœ¬..."
    cd $LOG_DIR
    python run_test.py test_config.yaml 2>&1 | tee test_output.log
    cd - > /dev/null
    
    print_success "æµ‹è¯•æ‰§è¡Œå®Œæˆ"
}

# åˆ†ææµ‹è¯•ç»“æœ
analyze_results() {
    print_info "åˆ†ææµ‹è¯•ç»“æœ..."
    
    # æ£€æŸ¥æµ‹è¯•æŠ¥å‘Š
    if [ -f "$TEST_DIR/test_report.json" ]; then
        # æå–å…³é”®ä¿¡æ¯
        TOTAL_TESTS=$(python -c "import json; data=json.load(open('$TEST_DIR/test_report.json')); print(data['summary']['total_tests'])")
        PASSED_TESTS=$(python -c "import json; data=json.load(open('$TEST_DIR/test_report.json')); print(data['summary']['passed_tests'])")
        FAILED_TESTS=$(python -c "import json; data=json.load(open('$TEST_DIR/test_report.json')); print(data['summary']['failed_tests'])")
        
        print_info "æµ‹è¯•ç»Ÿè®¡:"
        echo "   - æ€»æµ‹è¯•æ•°: $TOTAL_TESTS"
        echo "   - é€šè¿‡æµ‹è¯•: $PASSED_TESTS"
        echo "   - å¤±è´¥æµ‹è¯•: $FAILED_TESTS"
        
        if [ "$FAILED_TESTS" -eq 0 ]; then
            print_success "æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼"
        else
            print_warning "æœ‰ $FAILED_TESTS ä¸ªæµ‹è¯•å¤±è´¥"
        fi
    else
        print_warning "æœªæ‰¾åˆ°æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶"
    fi
    
    # ä¿å­˜ç³»ç»Ÿä¿¡æ¯
    nvidia-smi > $TEST_DIR/gpu_info.txt
    df -h > $TEST_DIR/disk_info.txt
    
    print_success "ç»“æœåˆ†æå®Œæˆ"
}

# ç”Ÿæˆæµ‹è¯•æ€»ç»“
generate_summary() {
    print_info "ç”Ÿæˆæµ‹è¯•æ€»ç»“..."
    
    cat > $TEST_DIR/test_summary.md << EOF
# æµ‹è¯•æ€»ç»“

## æµ‹è¯•ä¿¡æ¯
- æµ‹è¯•åç§°: $TEST_NAME
- æ•°æ®é›†: $DATASET
- æ£€æŸ¥ç‚¹: $CHECKPOINT_PATH
- æµ‹è¯•æ—¶é—´: $(date)

## é…ç½®å‚æ•°
- æ‰¹æ¬¡å¤§å°: $BATCH_SIZE
- å·¥ä½œè¿›ç¨‹: $NUM_WORKERS

## æ–‡ä»¶è·¯å¾„
- æµ‹è¯•ç»“æœ: $TEST_DIR
- æµ‹è¯•æ—¥å¿—: $LOG_DIR
- è¯¦ç»†æŠ¥å‘Š: $TEST_DIR/test_report.md

## ä¸‹ä¸€æ­¥
1. æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: cat $TEST_DIR/test_report.md
2. æŸ¥çœ‹æµ‹è¯•æ—¥å¿—: cat $LOG_DIR/test_output.log
3. åˆ†ææ€§èƒ½æŒ‡æ ‡: python scripts/analyze_performance.py $TEST_DIR
EOF
    
    print_success "æµ‹è¯•æ€»ç»“å·²ç”Ÿæˆ: $TEST_DIR/test_summary.md"
}

# é”™è¯¯å¤„ç†
handle_error() {
    print_error "æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯"
    
    if [ -d "$LOG_DIR" ]; then
        echo "é”™è¯¯æ—¶é—´: $(date)" >> $LOG_DIR/error_info.txt
        
        if [ -f "$LOG_DIR/test_output.log" ]; then
            tail -20 $LOG_DIR/test_output.log > $LOG_DIR/error.log
            print_info "é”™è¯¯æ—¥å¿—å·²ä¿å­˜: $LOG_DIR/error.log"
        fi
    fi
    
    exit 1
}

# ä¸»å‡½æ•°
main() {
    echo "å¼€å§‹æµ‹è¯•æµç¨‹..."
    
    check_environment
    setup_test_env
    setup_test_dirs
    check_dataset
    check_checkpoint
    create_test_config
    run_test
    analyze_results
    generate_summary
    
    echo "================================================"
    print_success "ğŸ‰ æµ‹è¯•æµç¨‹å®Œæˆï¼"
    echo ""
    echo "ğŸ“‹ æµ‹è¯•ç»“æœ:"
    echo "   - æµ‹è¯•ç›®å½•: $TEST_DIR"
    echo "   - æ—¥å¿—ç›®å½•: $LOG_DIR"
    echo ""
    echo "ğŸ“Š æŸ¥çœ‹ç»“æœ:"
    echo "   - æµ‹è¯•æŠ¥å‘Š: cat $TEST_DIR/test_report.md"
    echo "   - æµ‹è¯•æ€»ç»“: cat $TEST_DIR/test_summary.md"
    echo "   - æµ‹è¯•æ—¥å¿—: cat $LOG_DIR/test_output.log"
}

# è®¾ç½®é”™è¯¯å¤„ç†
trap handle_error ERR

# è¿è¡Œä¸»å‡½æ•°
main "$@"